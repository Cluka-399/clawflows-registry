# Lobster workflow: repeat-task-detector
# Analyze agent activity logs to find repetitive patterns
#
# Usage:
#   lobster run --file workflow.yaml --args-json '{"memory_path":"/data/clawd/memory","days_to_analyze":"14","min_occurrences":"3"}'
#
# Requires: jq
#
# LLM: Uses prompt step for pattern analysis and automation suggestions

name: repeat-task-detector
description: Analyze agent activity logs to find repetitive patterns that could be automated

args:
  memory_path:
    description: "Path to agent memory/logs directory"
    default: "/data/clawd/memory"
  days_to_analyze:
    description: "How many days of logs to scan"
    default: "14"
  min_occurrences:
    description: "Minimum times a pattern must appear to be reported"
    default: "3"

steps:
  - id: collect-logs
    command: |
      mem="${memory_path}"
      days="${days_to_analyze}"
      if [ ! -d "$mem" ]; then
        echo '{"error":"memory path not found","files":0,"content":""}'
        exit 0
      fi
      # Find recent daily log files (YYYY-MM-DD.md format)
      cutoff=$(date -d "-${days} days" +%Y-%m-%d 2>/dev/null || date -v-${days}d +%Y-%m-%d 2>/dev/null || echo "2020-01-01")
      files=$(find "$mem" -maxdepth 1 -name "????-??-??.md" -type f | sort -r | head -n "$days")
      count=$(echo "$files" | grep -c . || echo 0)
      if [ "$count" -eq 0 ] || [ -z "$files" ]; then
        echo '{"files":0,"lines":[]}'
        exit 0
      fi
      # Extract action-like lines from logs (bullets, commands, tasks)
      echo "$files" | xargs grep -hE '^\s*[-*•]\s+' 2>/dev/null \
        | sed 's/^[ \t]*[-*•][ \t]*//' \
        | grep -iE '(check|search|fetch|send|read|write|update|monitor|alert|email|calendar|deploy|push|pull|run|post|tweet|notify|scan|review)' \
        | jq -R '.' | jq -sc --argjson n "$count" '{files: $n, lines: .}'

  - id: find-patterns
    stdin: $collect-logs.stdout
    command: |
      cat > /tmp/lb_rt_logs.json
      min_occ="${min_occurrences}"
      jq -c --argjson min "$min_occ" '
        .files as $filecount |
        if (.lines | length) == 0 then
          {patterns: [], candidates: [], file_count: $filecount}
        else
          # Normalize lines: lowercase, strip specifics (dates, numbers, urls)
          [.lines[] |
            gsub("\\d{4}-\\d{2}-\\d{2}"; "DATE") |
            gsub("\\d{1,2}:\\d{2}"; "TIME") |
            gsub("https?://[^\\s]+"; "URL") |
            gsub("\\d+"; "N") |
            ascii_downcase
          ] |
          # Count occurrences of each normalized pattern
          group_by(.) | map({pattern: .[0], count: length}) |
          sort_by(-.count) |
          [.[] | select(.count >= $min)] |
          {
            patterns: .,
            candidates: [.[] | select(.count >= ($min * 2)) | . + {suggestion: "High frequency - strong automation candidate"}],
            file_count: $filecount
          }
        end
      ' /tmp/lb_rt_logs.json
      rm -f /tmp/lb_rt_logs.json

  - id: analyze-and-suggest
    stdin: $find-patterns.stdout
    prompt: |
      Analyze the repeated task patterns detected in agent activity logs.

      For each pattern found:
      1. Describe what the agent is repeatedly doing
      2. Estimate time spent per occurrence
      3. Rate automation potential: HIGH / MEDIUM / LOW
      4. For HIGH patterns, suggest a specific automation approach
         (cron job, Lobster workflow, shell script, etc.)
      5. Estimate time savings per week if automated

      Also identify:
      - Tasks that seem like they should be batched together
      - Patterns that suggest missing tools or integrations
      - Quick wins vs. longer-term automation projects

      Minimum occurrences threshold: ${min_occurrences}
    system: >
      You are a workflow automation consultant. Find repetitive work and design efficient automations.

